# Introducción a Python para las Finanzas (Last Update: March 2022)

## Descripción del Curso

Este curso proveerá al alumno conocimientos teóricos y prácticos de métodos analíticos que permiten transformar la información de una organización en inputs para la toma de decisiones con un enfoque cuantitativo. Los métodos analizados serán técnicas de Machine Learning, comenzando con algoritmos sencillos hasta técnicas más complejas que actualmente representan el estado del arte en esta área de estudio. Al final del curso los alumnos serán capaces de aplicar estas técnicas para distintos objetivos, permitiendo rentabilizar –social o económicamente-, el activo de datos disponible en una entidad.

## Objetivos
Este curso tiene como objetivo que los estudiantes sean capaces de comprender y aplicar métodos cuantitativos para transformar los datos disponibles en una organización en información valiosa, ya sea para ganar conocimiento sobre un suceso y/o apoyar la toma de decisiones basada en un racionamiento objetivo.

1. Promover una cultura para la extracción de conocimiento y toma de decisiones basada en datos. 
2. Identificar oportunidades de mejora con la aplicación de técnicas cuantitativas avanzadas, reconociendo y entendiendo las fuentes de datos existentes así como las necesidades de información para la apropiada aplicación de estos métodos.
3. Desarrollar habilidades de trabajo en equipo, así como de programación para la aplicación de las metodologías expuestas. LO 3.1
4. Desarrollar habilidades de síntesis y comunicación a través de la presentación y/o reporte ejecutivo de las principales conclusiones de un proyecto de aplicación de métodos cuantitativos, desde la identificación del problema hasta la obtención de resultados.


## Metodología

Clases teóricas y prácticas en las que el profesor cubrirá los contenidos de forma expositiva, incentivando la participación de los alumnos. Cada tópico será primero abordado en forma teórica para luego pasar a su aplicación mediante la programación en Python. Las sesiones de ayudantía serán definidas en el semestre, y estarán enfocadas en la parte de programación del curso.

## Contenidos

1. Introducción.
   
2. Contexto y definiciones en Inteligencia Artificial.
    
3. Ejemplos de casos de uso de Machine Learning.

4. Estructura general de un proyecto de Machine Learning.

5. Convexidad y sus implicancias en optimización.

6. Algoritmos de optimización para Machine Learning.

7. Algoritmos para Clasificación.
 
8. Algoritmos para Regresión.

9. Métodos Ensamblados.

10. Interpretabilidad en modelos complejos.


## Lecturas Recomendadas

• [Acito and Khatri, 2014] Acito, F. and Khatri, V. (2014). Business analytics: Why now and what next? Business Horizons, 57(5):565 – 570.
• [Bergstra and Bengio, 2012] Bergstra, J. and Bengio, Y. (2012). Random search for hyperparameter optimization. J. Mach. Learn. Res., 13(1):281–305.
• [Bergstra et al., 2011] Bergstra, J. S., Bardenet, R., Bengio, Y., and Kégl, B. (2011). Algorithms for hyper-parameter optimization. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F., and
• Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 24, pages 2546– 2554. Curran Associates, Inc.
• [Bertsekas, 1999] Bertsekas, D. (1999). Nonlinear Programming. Athena Scientific.
• [Bishop, 2006] Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.
• [Bottou et al., 2016] Bottou, L., Curtis, F. E., and Nocedal, J. (2016). Optimization Methods for Large-Scale Machine Learning. ArXiv e-prints.
• [Brochu et al., 2010] Brochu, E., Cora, V. M., and de Freitas, N. (2010). A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. ArXiv e-prints.
• [Cauchy, 1847] Cauchy, A. (1847). Méthode générale pour la résolution des systems d’équations  simultanées. Comp. Rend. Sci. Paris, 25(1847):536–538.
• [Chen et al., 2011] Chen, S., Härdle, W. K., and Moro, R. A. (2011). Modeling default risk with support vector machines. Quantitative Finance, 11(1):135– 154.
• [Chollet, 2017] Chollet, F. (2017). Deep Learning with Python. Manning Publications Co.,Greenwich, CT, USA, 1st edition.
• [Cireşan et al., 2013] Cireşan, D. C., Giusti, A., Gambardella, L. M., and Schmidhuber, J. (2013). Mitosis detection in breast cancer histology images with deep neural networks. In Mori, K., Sakuma, I., Sato, Y., Barillot, C., and Navab, N., editors, Medical Image Computing and Computer-Assisted Intervention – MICCAI 2013, pages 411–418, Berlin, Heidelberg. Springer Berlin Heidelberg.
• [Dozat, 2015] Dozat, T. (2015). Incorporating nesterov momentum into Adam.
• [Duchi et al., 2011] Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159.
• [Elbasiony et al., 2013] Elbasiony, R. M., Sallam, E. A., Eltobely, T. E., and Fahmy, M. M. (2013). A hybrid network intrusion detection framework based on random forests and weighted k-means. Ain Shams Engineering Journal, 4(4):753 – 762.
• [Friedman, 2001] Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Ann. Statist., 29(5):1189–1232.
• [Goodfellow et al., 2016] Goodfellow, I., Bengio, Y., and Courville, A. (2016).Deep Learning. MIT Press. http://www.deeplearningbook.org.
• [Gron, 2017] Gron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, Inc., 1st edition.
• [Gregorutti et al., 2017] Gregorutti, B., Michel, B., and Saint-Pierre, P. (2017). Correlation and variable importance in random forests. Statistics and Computing, 27(3):659–678.
• [Guyon et al., 2002] Guyon, I., Weston, J., Barnhill, S., and Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46(1):389–422
• [Hinton et al., 2012] Hinton, G., Srivastava, N., and Swersky, K. (2012). Overview of mini-batch gradient descent. Lectures on Neural Networks for Machine Learning.
• [Holsapple et al., 2014] Holsapple, C., Lee-Post, A., and Pakath, R. (2014). A unified foundation for business analytics. Decision Support Systems, 64:130 – 141.
• [Iyyer et al., 2014] Iyyer, M., Enns, P., Boyd-Graber, J. L., and Resnik, P. (2014). Political ideology detection using recursive neural networks. In ACL.
• [Kiefer and Wolfowitz, 1952] Kiefer, J. and Wolfowitz, J. (1952). Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, 23(3):462–466.
• [Kingma and Ba, 2014] Kingma, D. P. and Ba, J. (2014). Adam: A Method for Stochastic Optimization. ArXiv e-prints.
• [Lv et al., 2015] Lv, Y., Duan, Y., Kang, W., Li, Z., and Wang, F. (2015). Traffic flow prediction with big data: A deep learning approach. IEEE Transactions on Intelligent Transportation Systems, 16(2):865–873.
• [Lunderberg and Lee, 2017] Lunderberg S., Lee S. (2017). A Unified Approach to Interpreting Model Predictions.
• [Marsland, 2014] Marsland, S. (2014). Machine Learning: An Algorithmic Perspective, Second Edition. Chapman & Hall/CRC, 2nd edition.
• [Nesterov, 1983] Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate O(1/sqr(k)). Soviet Mathematics Doklady, 27:372–376.
• [Perlich et al., 2014] Perlich, C., Dalessandro, B., Raeder, T., Stitelman, O., and Provost, F. (2014). Machine learning for targeted display advertising: transfer learning in action. Machine Learning, 95(1):103–127.
• [Polyak, 1964] Polyak, B. (1964). Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1 – 17.
• [Raschka, 2015] Raschka, S. (2015). Python Machine Learning. Packt Publishing.
• [Rasmussen, 2004] Rasmussen, C. E. (2004). Gaussian Processes in Machine Learning, pages 63–71. Springer Berlin Heidelberg, Berlin, Heidelberg.
• [Ribeiro et al., 2016] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). "Why should i trust you?": Explaining the predictions of any classifier. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pages 1135–1144, New York, NY, USA. ACM.
• [Robbins and Monro, 1951] Robbins, H. and Monro, S. (1951). A stochastic approximation method. Ann. Math. Statist., 22(3):400–407.
• [Ruder, 2016] Ruder, S. (2016). An overview of gradient descent optimizationalgorithms. CoRR, abs/1609.04747.
• [Saxena and Srinivasan, 2013] Saxena, R. and Srinivasan, A. (2013). Business Analytics: A Practitioner’s Guide. Springer New York, New York, NY.
• [Smola and Schölkopf, 2004] Smola, A. J. and Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and Computing, 14(3):199–222.
• [Sutskever et al., 2013] Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In Dasgupta, S. and McAllester, D., editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1139–1147, Atlanta, Georgia, USA. PMLR.
• [Tran et al., 2018] Tran, P. H., Tran, K. P., Huong, T. T., Heuchenne, C., Hien- Tran, P., and Le, T. M. H. (2018). Real time data-driven approaches for credit card fraud detection. In Proceedings of the 2018 International Conference on E-Business and Applications, ICEBA 2018, pages 6–9, New York, NY, USA. ACM.
• [Tsai and Lu, 2009] Tsai, C.-F. and Lu, Y.-H. (2009). Customer churn prediction by hybrid neural networks. Expert Systems with Applications, 36(10):12547 – 12553.
• [Zeiler, 2012] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. ArXiv eprints.
